# This file is used to configure logging and agents behaviour.
#
# The first part consists of Wandb info used to log experiments.
# Changing it adjusts the way logging is stored and displayed.
#
# The second part (config) is used to change hyperparameter settings of agents.
# Changing it adjusts the way agents behave and learn.
project: "RL CartPole"
name: "Rainbow"
dir: "../logs"
notes: "Testing algorithm functionality, logging and model saving -- Rainbow (DQN, Double, Dueling, PER, N-step)."
mode: "online"
monitor_gym: "False"
config:
  # Environment, logging and saving control
  environment: "CartPole-v1"                # Environment to use
  algorithm: "Rainbow"                      # What kind of algorithm to use?
  save_dir: "../models/"                    # Where to save model?
  save_name: "RainbowCartPole"              # Model name
  save_interval: 20                         # How many previous episodes will be used to calculate mean reward?
  total_steps: 50000                        # For how many steps will the agent train?
  episode_steps: 500                        # How many steps before the episode is terminated?
  # Algorithm hyperparameters
  memory_size: 16384                        # How many steps can fit into the memory?
  learning_rate_policy: 0.0005               # Learning rate for Q-Network
  gamma: 0.99                               # Discount factor
  epsilon_decay: 0.999                      # Only temporary! Later, noisy nets will be used
  warmup_steps: 500                         # How many steps before agents starts optimising?
  batch_size: 64                            # How big will the batches used in optimization be?
  tau: 0.01                                 # Polyak parameter
  alpha: 0.4                                # How much prioritization is used?
  beta: 0.6                                 # Importance sampling negative exponent
  n_step: 4                                 # How many steps are used in multistep update?