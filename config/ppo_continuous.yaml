# This file is used to configure logging and agents behaviour.
#
# The first part consists of Wandb info used to log experiments.
# Changing it adjusts the way logging is stored and displayed.
#
# The second part (config) is used to change hyperparameter settings of agents.
# Changing it adjusts the way agents behave and learn.
project: "ReinforcementLearning"
name: "PPO MountainCarContinuous-v0"
dir: "../logs"
notes: "Training of MountainCar -- everything should work properly."
mode: "online"
monitor_gym: "False"
config:
  environment: "MountainCarContinuous-v0"   # Environment to use
  model: "continuous"                       # What kind of model to use?
  save_dir: "../models/"                    # Where to save model?
  save_name: "MountainCar.pth"              # Model name
  episodes: 200                             # For how many episodes will agent train?
  max_steps: 999                            # How many steps before the episode is terminated?
  gamma: 0.999                              # Discount factor for future rewards (long horizon)
  lambda: 0.95                              # GAE tradeoff parameter
  ppo_epochs: 10                            # How many epochs to train on each batch?
  batch_size: 1024                          # How many steps are in each batch? Smaller for faster updates
  clip_epsilon: 0.2                         # How much is the policy clipped?
  learning_rate_actor: 0.0003               # Learning rate for actor head
  learning_rate_critic: 0.001               # Learning rate for critic head
  value_loss_coef: 0.5                      # How much is the value loss weighted?
  entropy_coef: 0.05                        # How much is the entropy loss weighted? (encourages exploration)
